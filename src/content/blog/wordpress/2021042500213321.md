---
title: "ICTSC2020 k8s運用解説 後編：運用編"
description: "ICTSC2020 k8s運用解説 後編：運用編"
tags: [ICTSC2020,Tips,アプリケーション関連,サーバー関連,ネットワーク関連]
pubDate: 2021-04-25T00:21:16
slug: "2021/04/25/ICTSC2020 k8s運用解説 後編：運用編"
draft: false
renderer: "html"
sticky: false
---


<p>こんにちは。<a href="https://twitter.com/takemioIO">@takemioIO</a>です。<br> この記事は <a href="https://blog.icttoracon.net/2021/04/25/ictsc2020-k8s%e9%81%8b%e7%94%a8%e8%a7%a3%e8%aa%ac-%e5%89%8d%e7%b7%a8%ef%bc%9a%e6%a7%8b%e7%af%89%e3%81%a8%e6%a7%8b%e6%88%90/">ICTSC2020 k8s運用解説、前編:構築と構成</a>の後編 にあたります。<br> ここでは以前の記事ではk8sを構築したことについてと全体像について述べました。ここではその構築したk8sをどのように利用したのか、それらを通じた知見を述べます。</p>



<h3>CDについて</h3>



<figure class="wp-block-image"><img decoding="async" src="https://i.imgur.com/zLXPm3H.png.webp" alt="" /></figure>



<p>こんにちは。CI/CDに関する部分を担当した sakuraiです<br>
今年は<a href="https://argoproj.github.io/argo-cd/">ArgoCD</a>と<a href="https://github.com/roboll/helmfile">Helmfile</a>を利用してクラスタごとにスコアサーバー(コンテストサイト)などをデプロイする運用をしました。<br>
</p>



<h4>背景</h4>



<p>クラスタチームではKubernetesクラスタを3つ運用し、次のような位置づけとしています。</p>



<ul><li>wspクラスタ(監視系、ダッシュボードなど)</li><li>devクラスタ(開発用テスト環境)</li><li>prdクラスタ(コンテスト参加者へ提供する本番環境)</li></ul>



<p>各クラスタへのアプリケーションのデプロイを行うためには、そのクラスタに対して<code>kubectl apply -f hogehoge.yaml</code>といったコマンドを打つ必要があります。しかし、これを手作業で行うことは</p>



<ul><li>単純に手間</li><li>人為的なミスが起こりうる</li><li>アプリケーション側の人間がクラスタを触る必要がある</li></ul>



<p>ということがあります。そこで、クラスタへのデプロイを自動化したりクラスタチーム以外がk8s上にアプリケーションをデプロイするときの動線を整備したりすることによって、k8sとその上のアプリケーションの運用を継続的に行えることを目指しました。</p>



<h4>Helmfile</h4>



<p>まず初めに、これまでスクリプトでごり押されていたマニフェスト群をテンプレート化を行いました。テンプレート化にはKustomizeと<a href="https://helm.sh/docs/chart_template_guide/">Helm</a>が候補となりましたが、環境変数の変更のしやすさの観点からHelmを使用することにしました。また、要件としてアプリケーションを各クラスタへデプロイする際、環境変数を変えることでConfigMapやimageを変更できる必要がありました。このテンプレート化では例えば、</p>



<blockquote class="wp-block-quote"><p>deployment.yaml</p></blockquote>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: {{ .Values.__VAR__NAMESPACE }}
...</code></pre></div>


<blockquote class="wp-block-quote"><p>values.yaml</p></blockquote>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>__VAR__NAMESPACE: &quot;scoreserver&quot;</code></pre></div>


<p>というようにマニフェストの一部を変数として、その変数に対応する内容を記したファイルを用意することで目的のマニフェストを出力することができます。</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code># helm template --values values.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: scoreserver
...</code></pre></div>


<p>さらに、この変数の内容(values.yaml)を各クラスタと紐づけるために<a href="https://github.com/roboll/helmfile">Helmfile</a>を利用しました。HelmfileではHelmのテンプレートに加えて環境を定義することができます。ここでdevelop/productionというように環境を定義し、環境変数も紐づけていきます。</p>



<blockquote class="wp-block-quote"><p>helmfile.yaml</p></blockquote>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>environments:
  workspace:
    values:
    - environment/workspace/values.yaml
  develop:
    values:
    - environment/develop/values.yaml
  production:
    values:
    - environment/production/values.yaml
...</code></pre></div>


<p>単純な置き換えとしては次のようになります。</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>helm template --values environment/production/values.yaml

↓

helmfile --environment production template .</code></pre></div>


<h4>Argo CD</h4>



<p>クラスタへのアプリケーションデプロイの自動化ツールとして、<a href="https://argoproj.github.io/argo-cd/">Argo CD</a>を利用しました。Argo CDはGitHub上のマニフェストを参照して、アプリケーションのデプロイを行うことができます。また、GitHub上でそのマニフェストが変更された場合に自動でデプロイを行うことができます。したがって、Argo CD上でデプロイ設定を行った後はGitHub上でマニフェストを管理することでデプロイを行うことができるため、アプリケーション管理者がクラスタへ触る必要がなくなります。</p>



<p>(Argo CD自体の導入はとても簡単なので内容としては割愛します。GUIで設定できるし適当に使いたいときもおすすめできそう。)</p>



<p>Argo CDはhelmなどのテンプレートエンジンに対応していますが、Helmfileには対応してないためプラグインとして追加します。</p>



<blockquote class="wp-block-quote"><p>deploy.yaml</p></blockquote>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>spec:
  template:
    spec:
      # 1. Define an emptyDir volume which will hold the custom binaries
      volumes:
      - name: custom-tools
        emptyDir: {}
      # 2. Use an init container to download/copy custom binaries into the emptyDir
      initContainers:
      - name: download-tools
        image: alpine:3.8
        command: &#91;sh, -c]
        args:
        - wget -qO /custom-tools/helmfile https://github.com/roboll/helmfile/releases/download/v0.128.0/helmfile_linux_amd64 &amp;&amp; chmod +x /custom-tools/*
        volumeMounts:
        - mountPath: /custom-tools
          name: custom-tools
      containers:
      - name: argocd-repo-server
        volumeMounts:
        - mountPath: /usr/local/bin/helmfile
          name: custom-tools
          subPath: helmfile</code></pre></div>

<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>kubectl -n argocd patch deploy/argocd-repo-server -p &quot;$(cat deploy.yaml)&quot;</code></pre></div>


<p>アプリケーション情報の登録はCUIまたはGUIで行うことができるが、こちらもテキストで管理が可能です。</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: scoreserver-production
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: production
  source:
    repoURL: /* CENSORED */
    path: scoreserver
    targetRevision: master
    plugin:
      name: helmfile
      env:
        - name: ENV
          value: production
  destination:
    name: kubernetes-prd　#ここでクラスタを指定する。Argo CDが動いているクラスタ以外は別途事前登録する必要がある。
    namespace: scoreserver-production 
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true</code></pre></div>


<p>Argo CDにはadminユーザがデフォルトで設定されていますが、ユーザの追加を行ったり権限設定を行うことが可能です。(<a href="https://argoproj.github.io/argo-cd/operator-manual/rbac/">設定方法</a>)<br>
ICTSCではprdクラスタへの変更を制限して、本番環境への破壊や意図しない変更を防ぐようにしています。</p>



<h4>Argo CD notifications</h4>



<p>デプロイ結果をSlackに通知するために<a href="https://github.com/argoproj-labs/argocd-notifications">Argo CD notifications</a>を使いました。<br>
(比較的最近v1.0~になって変更が辛かったデスネ)</p>



<p>ドキュメントがアレなんですが、最低限以下だけすれば動きそうです。(別途Slack側でOauth tokenの発行は必要)</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code># kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-notifications/v1.0.2/manifests/install.yaml
# kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-notifications/v1.0.2/catalog/install.yaml
# kubectl edit cm -n argocd argocd-notifications-cm

data:
  defaultTriggers: |
    - on-deployed
    - on-health-degraded
    - on-sync-failed
    - on-sync-running
    - on-sync-status-unknown
    - on-sync-succeeded</code></pre></div>

<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code># kubectl edit secret -n argocd argocd-notifications-secret

stringData:
  slack-token: CENSORED</code></pre></div>


<p>以下のような通知が届くようになります。</p>



<figure class="wp-block-image"><img decoding="async" src="https://i.imgur.com/hsxRF4v.png.webp" alt="" /></figure>



<h3>監視基盤</h3>



<p>こんにちは、監視基盤を担当した梅田<a href="https://twitter.com/x86taka">@x86taka</a>です。</p>



<p>今回は監視基盤として行ったことを説明します。</p>



<p>まず初めに、今年の監視基盤の構成を説明します。</p>



<h4>構成</h4>



<p>まず、最初に監視対象です。</p>



<p>監視対象はNTT中央研修センタをメインに行いました。</p>



<ul><li>NTT中央研修センタ<ul><li>サーバ <ul><li>S7 * 2</li><li>m4 * 2</li><li>RH1288 * 4</li></ul></li><li>ネットワーク機器<ul><li>MX5</li><li>SRX1500</li><li>SN2410</li></ul></li></ul></li><li>さくらクラウド<ul><li>k8sクラスタ<ul><li>prd クラスタ</li><li>wsp クラスタ</li></ul></li></ul></li></ul>



<figure class="wp-block-image"><img decoding="async" src="https://i.imgur.com/uqZEQ8K.png.webp" alt="" /></figure>



<p>NTT中央研修センタとさくらクラウドの構成は、前回の記事に詳細にかかれています。<br>
</p>



<figure class="wp-block-image"><img decoding="async" src="https://i.imgur.com/huIHPkH.png.webp" alt="" /></figure>



<p>また、これらの監視に利用したコンポーネントは以下の通りです。</p>



<ul><li>分析
<ul><li>Elasticsearch</li></ul></li><li>データ収集
<ul><li>Logstash</li><li>Prometheus</li><li>Elastiflow</li><li>Zabbix</li></ul></li><li>可視化
<ul><li>Kibana</li><li>Grafana</li></ul></li><li>その他
<ul><li>AlertManager</li></ul></li></ul>



<p>具体的にこれらのコンポーネントを、どのように利用したのかという話をしたいと思います。</p>



<h4>Hardware監視</h4>



<p>ここでは、サーバのIPMIから得られるデータやネットワーク機器(SNMP)のデータの監視についてです。</p>



<p>今回は、Zabbixを利用しNTT中央研修センタにある合計11台のホストを監視しました。</p>



<p>使用したテンプレートも含め、以下のスクリーンショットを載せておきます。</p>



<figure class="wp-block-image"><img decoding="async" src="https://i.imgur.com/jsm4iby.png.webp" alt="" /></figure>



<p>IPMIからは、サーバのハードウェアの状態の監視を行いました。</p>



<p>IPMIのデータ取得は、Zabbixのデフォルトの設定では行えません。<br>
ENVに<code>ZBX_IPMIPOLLERS=1</code> のように、0以上の値を設定することにより取得できるようになります。</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>            - name: ZBX_IPMIPOLLERS
              value: &quot;1&quot;</code></pre></div>


<p>今回は準備期間中に、HDDの故障が発生したりしていましたので重要な監視になりました。</p>



<p>(Problemのメッセージ)</p>



<p><code>hardDisk [1] status major</code></p>



<p>また、ネットワーク機器はSNMPによる監視, メトリクスの取得を行いました。</p>



<p>後述する、GrafanaでZabbixで取得した、対外トラフィックの可視化を行いました。</p>



<h4>サービス監視</h4>



<p>主にPrometheusを利用し、Grafanaで可視化を行いました。</p>



<figure class="wp-block-image"><img decoding="async" src="https://i.imgur.com/iABoRoo.png.webp" alt="" /></figure>



<h4>Prometheus</h4>



<p>Prometheusでは Node-expoter, Ceph-expoter, BlackBox-expoterなどを利用しデータの収集を行いました。</p>



<p>Prometheusのデータの永続化についてです。<br>
監視項目が多い場合かなりのストレージを使うため、注意が必要でした。</p>



<p>ICTSC2020の場合、10日間で20GBのストレージを消費しました。</p>



<p>また、PrometheusのDBのサイズが肥大化し起動に時間がかかるようになっていました。<br>
長期の運用を考える場合に考え直さなければならない部分だと思っています。</p>



<h4>Deploy</h4>



<p>ICTSC2020ではArgoCDによるデプロイを行っています。<br>
しかし、ConfigMapに書かれている設定の変更時にPodの再起動が行われません。</p>



<p>そのため、DeployのアノテーションにConfigMapのhash値をいれておき、ConfigMapに変更があったときのみPodのUpdateが行われるようにしました。</p>



<p>このような形です。</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>      annotations:
        checksum/config_volume: {{ $.Files.Get &quot;templates/prometheus-configmap.yaml&quot;| sha256sum }}
        checksum/blackbox_volume: {{ $.Files.Get &quot;templates/blackbox_exporter-configmap.yaml&quot;| sha256sum }}
        checksum/ceph_target_volume: {{ $.Files.Get &quot;templates/ceph-exporter-target.yaml&quot;| sha256sum }}
        checksum/node_volume: {{ $.Files.Get &quot;templates/node_exporter-configmap.yaml&quot;| sha256sum }}</code></pre></div>


<h4>Grafana</h4>



<p>Grafanaでは、DatasourceにPrometheus, Zabbixを利用し可視化していました。</p>



<p>全体で3つGrafanaが存在したため、Dashboardのjsonをk8sのConfigMapで管理しています。</p>



<h4>改善点</h4>



<p>ICTSCではConfigMapの定義yamlに、すべてのdashboardのjsonが以下のように一つのファイルに書かれています。</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>---
apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: grafana-import-dashboards
  namespace: monitoring
data:
  grafana-net-2-dashboard.json: |
    {
      &quot;__inputs&quot;: &#91;{
        &quot;name&quot;: &quot;DS_PROMETHEUS&quot;,
        &quot;label&quot;: &quot;Prometheus&quot;,
        &quot;description&quot;: &quot;&quot;,
        &quot;type&quot;: &quot;datasource&quot;,
        &quot;pluginId&quot;: &quot;prometheus&quot;,
        &quot;pluginName&quot;: &quot;Prometheus&quot;
      }],
      &quot;__requires&quot;: &#91;{
        &quot;type&quot;: &quot;panel&quot;,
        &quot;id&quot;: &quot;singlestat&quot;,
        &quot;name&quot;: &quot;Singlestat&quot;,
        &quot;version&quot;: &quot;&quot;
      }, {
        &quot;type&quot;: &quot;panel&quot;,
        &quot;id&quot;: &quot;text&quot;,
        &quot;name&quot;: &quot;Text&quot;,
        &quot;version&quot;: &quot;&quot;
      }, {
      .........</code></pre></div>


<p>しかし、この状態だとdashboardの変更した際にConfigMapの定義ファイルとjsonが同じところにあるため書き換えが大変でした。</p>



<p>そのため、jsonとConfigMapの定義を分離することにしました。</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: grafana-import-dashboards
  namespace: monitoring
data:
  grafana-net-2-dashboard.json: |-
{{ .Files.Get &quot;dashboard/grafana-net-2-dashboard.json&quot; | indent 4}}</code></pre></div>


<p>templateに置かれているConfigMap定義から、別ディレクトリに置かれているjsonを読み込む形に変更しました。</p>



<p>その際に<code>indent 4</code>　をつけることによって、yamlに読み込まれた際のインデントを気にする必要がなくなります。</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>  k8s-dashboard.json: |-
{{ .Files.Get &quot;dashboard/k8s-dashboard.json&quot; | indent 4}}</code></pre></div>


<p>また、サーバの監視にGrafana.comに公開されているdashboardを利用しました。</p>



<p><a href="https://grafana.com/grafana/dashboards/11074">https://grafana.com/grafana/dashboards/11074</a></p>



<p>上にある公開されているDashboardの一部において、jsonの定義でConfigMapのサイズ制限を超えてしまう問題が発生しました。<br>
GrafanaのAPIを利用して、Grafana.comから取得したデータのUploadを行い回避を行いました。</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>grafana_host=&quot;http://grafana:3000&quot;;
grafana_cred=&quot;${GF_ADMIN_USER}:${GF_ADMIN_PASSWORD}&quot;;
grafana_datasource=&quot;prometheus&quot;;
ds=(2842 1860 11074);
for d in &quot;${ds&#91;@]}&quot;; do
  echo -n &quot;Processing $d: &quot;;
  j=$(curl -s -k -u &quot;$grafana_cred&quot; $grafana_host/api/gnet/dashboards/&quot;$d&quot; | jq .json);
  echo &quot;{\&quot;dashboard\&quot;:$j,\&quot;overwrite\&quot;:true, \
        \&quot;inputs\&quot;:&#91;{\&quot;name\&quot;:\&quot;DS_PROMETHEUS\&quot;,\&quot;type\&quot;:\&quot;datasource\&quot;, \
        \&quot;pluginId\&quot;:\&quot;prometheus\&quot;,\&quot;value\&quot;:\&quot;$grafana_datasource\&quot;}]}&quot; \
  | curl -s -k -u &quot;$grafana_cred&quot; -XPOST -H &quot;Accept: application/json&quot; \
    -H &quot;Content-Type: application/json&quot; \
    $grafana_host/api/dashboards/import -d &quot;@-&quot; ;
  echo &quot;&quot; ;
done</code></pre></div>


<p>今回は行いませんでしたが、Grafana.comのようなPrivateなDashbordの公開場所を設けるなど、<br>
k8sのConfigMapに書き込まない方がよいと感じました。</p>



<p>また、以前公開しているPomeriumの記事に関連してGrafanaの認証をGitHubで行えるようにしています。</p>



<p>以下のような設定をgrafana.iniに書くことによってGitHub OAuth2を有効にしました</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>    &#91;auth.github]
    enabled = true
    allow_sign_up = true
    client_id = {{ .Values.github_clientid }}
    client_secret = {{ .Values.github_client_secret }}
    scopes = user:email,read:org
    auth_url = https://github.com/login/oauth/authorize
    token_url = https://github.com/login/oauth/access_token
    api_url = https://api.github.com/user
    team_ids = xxxxxxx
    allowed_organizations = ictsc</code></pre></div>


<p>GitHub Organizationで利用している場合、team_idsという設定項目でGitHub Organizationの特定のTeamのみ利用できるといった指定をすることができます。</p>



<p>team_idsというものは、チーム名ではなくGithubAPIから取得できる数字のIDを書かなければなりません。</p>



<p>GitHubのwebからは取得できないため、curlで取得する必要があります。</p>



<h4>GrafanaのSession管理</h4>



<p>また、Grafanaはセッション管理などにSQLiteを利用しています。</p>



<p>Grafanaをレプリカを行って運用する場合にはDBのLockがかかってしまうことがあります。</p>



<p>具体的に、DBのLockがかかるとGrafanaから突然ログアウトされるような現象が発生します。</p>



<p>そのため、MySQLサーバなどを用意しgrafana.iniで以下のようにデータベース接続の設定を行うことで解消します。<br>
databaseに接続情報を設定し、<code>[session]</code> をmysql変更します。</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>&#91;database]
    # You can configure the database connection by specifying type, host, name, user and password
    # as seperate properties or as on string using the url propertie.
    # Either &quot;mysql&quot;, &quot;postgres&quot; or &quot;sqlite3&quot;, it's your choice
    type = mysql
    host = helm-grafana-mysql:3306
    name = grafana
    user = ictsc
    # If the password contains # or ; you have to wrap it with trippel quotes. Ex &quot;&quot;&quot;#password;&quot;&quot;&quot;
    password = hogehoge
&#91;session]
     # Either &quot;memory&quot;, &quot;file&quot;, &quot;redis&quot;, &quot;mysql&quot;, &quot;postgres&quot;, default is &quot;file&quot;
     provider = mysql </code></pre></div>


<h4>ネットワーク監視</h4>



<p>次は、ネットワーク監視についてです。</p>



<p>主に、DataSourceはSNMP, sflowからデータを取得を行いました。</p>



<p>監視対象は、NTT中央研修センターにあるネットワーク機器です。<br>
繰り返しになりますが、列挙しておきます。</p>



<ul><li>ネットワーク機器<ul><li>MX5</li><li>SRX1500</li><li>SN2410</li></ul></li></ul>



<p>これら3台の機器を、ZabbixからSNMPでデータ取得を行いました。</p>



<p>これらの機器のICTSC2020での使用用途を簡単に説明します。</p>



<ul><li>MX5<br>
HomeNOC様とのBGPフルルートを受け取っているルータです。<br>
2拠点と接続し冗長化を行っています。</li><li>SRX1500<br>
MX5の配下に接続されている、Firewallです。</li><li>SN2410<br>
サーバ間の通信など、データ通信のコアスイッチとして利用しています。</li></ul>



<p>これらの機器から取得したデータの可視化を行いました。</p>



<p>まず、HomeNOC様との対外トラフィックの可視化についてです。</p>



<p>以下は、取得したデータをZabbixのScreenで表示させたのものです。</p>



<figure class="wp-block-image"><img decoding="async" src="https://i.imgur.com/vNasYPt.png.webp" alt="" /></figure>



<p>今回はGrafanaで様々な監視のdashboardを扱っていますので、GrafanaでZabbixのデータを表示を行います。</p>



<p>GrafanaからZabbixのデータを表示するために、以下のPluginのインストールを行います。<br>
https://grafana.com/grafana/plugins/alexanderzobnin-zabbix-app/</p>



<p>この際、zipファイルからインストール作業をする必要はなく、k8sのマニフェストからENVでインストールするプラグインを指定できます。</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>          - name: GF_INSTALL_PLUGINS
            value: &quot;alexanderzobnin-zabbix-app&quot;</code></pre></div>


<p>今回、ZabbixをDatasourceとしたDashboardとして対外トラフィックの可視化を行いました。</p>



<p>以下は、本戦二日間の実際のDashboardになります。</p>



<figure class="wp-block-image"><img decoding="async" src="https://i.imgur.com/RHYW87h.png.webp" alt="" /></figure>



<h4>Elastiflow</h4>



<p><a href="https://github.com/robcowart/elastiflow">Elastiflow</a>を利用したflow情報の可視化です。</p>



<p>構築はElastiflowのdocker-composeファイルを参考にk8sのtemplateを書いて構築をしました。<br><a href="https://github.com/robcowart/elastiflow/blob/master/docker-compose.yml"> https://github.com/robcowart/elastiflow/blob/master/docker-compose.yml</a></p>



<p>参考までに、作成したelastiflow-logstash用のファイルを載せておきます。</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; title: ; notranslate" title=""><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: elastiflow
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: elastiflow
  replicas: 2
  template:
    metadata:
      labels:
        app: elastiflow
    spec:
      containers:
        - name: elastiflow
          image: robcowart/elastiflow-logstash:4.0.1
          env:
            - name: LS_JAVA_OPTS
              value: &quot;-Xms3g -Xmx3g&quot;
            - name: ELASTIFLOW_AGENT_ID
              value: &quot;elastiflow&quot;
            - name: ELASTIFLOW_GEOIP_CACHE_SIZE
              value: &quot;16384&quot;
            - name: ELASTIFLOW_GEOIP_LOOKUP
              value: &quot;true&quot;
            - name: ELASTIFLOW_ASN_LOOKUP
              value: &quot;true&quot;
            - name: ELASTIFLOW_OUI_LOOKUP
              value: &quot;true&quot;
            - name: ELASTIFLOW_POPULATE_LOGS
              value: &quot;true&quot;
            - name: ELASTIFLOW_KEEP_ORIG_DATA
              value: &quot;true&quot;
            - name: ELASTIFLOW_DEFAULT_APPID_SRCTYPE
              value: &quot;__UNKNOWN&quot;
            - name: ELASTIFLOW_RESOLVE_IP2HOST
              value: &quot;true&quot;
            - name: ELASTIFLOW_NAMESERVER
              value: &quot;127.0.0.1&quot;
            - name: ELASTIFLOW_DNS_HIT_CACHE_SIZE
              value: &quot;25000&quot;
            - name: ELASTIFLOW_DNS_HIT_CACHE_TTL
              value: &quot;900&quot;
            - name: ELASTIFLOW_DNS_FAILED_CACHE_SIZE
              value: &quot;75000&quot;
            - name: ELASTIFLOW_DNS_FAILED_CACHE_TTL
              value: &quot;3600&quot;
            - name: ELASTIFLOW_ES_HOST
              value: &quot;elasticsearch:9200&quot;
            - name: ELASTIFLOW_NETFLOW_IPV4_PORT
              value: &quot;2055&quot;
            - name: ELASTIFLOW_NETFLOW_UDP_WORKERS
              value: &quot;4&quot;
            - name: ELASTIFLOW_NETFLOW_UDP_QUEUE_SIZE
              value: &quot;4096&quot;
            - name: ELASTIFLOW_NETFLOW_UDP_RCV_BUFF
              value: &quot;33554432&quot;

            - name: ELASTIFLOW_SFLOW_IPV4_PORT
              value: &quot;6343&quot;
            - name: ELASTIFLOW_SFLOW_UDP_WORKERS
              value: &quot;4&quot;
            - name: ELASTIFLOW_SFLOW_UDP_QUEUE_SIZE
              value: &quot;4096&quot;
            - name: ELASTIFLOW_SFLOW_UDP_RCV_BUFF
              value: &quot;33554432&quot;

            - name: ELASTIFLOW_IPFIX_UDP_IPV4_PORT
              value: &quot;4739&quot;
            - name: ELASTIFLOW_IPFIX_UDP_WORKERS
              value: &quot;2&quot;
            - name: ELASTIFLOW_IPFIX_UDP_QUEUE_SIZE
              value: &quot;4096&quot;
            - name: ELASTIFLOW_IPFIX_UDP_RCV_BUFF
              value: &quot;33554432&quot;
---
apiVersion: v1
kind: Service
metadata:
  name: elastiflow
  namespace: monitoring
  annotations:
    metallb.universe.tf/address-pool: privateIP
spec:
  ports:
    - name: netflow-port
      port: 2055
      protocol: UDP
      targetPort: 2055
    - name: sflow-port
      port: 6343
      protocol: UDP
      targetPort: 6343
    - name: ipfix-port
      port: 4739
      protocol: UDP
      targetPort: 4739
  selector:
    app: elastiflow
  type: LoadBalancer</code></pre></div>


<p>ElastiflowのDashboardは以下にあるjsonファイルをKibanaからimportを行うことによって、みれるようになります。</p>



<p><a href="https://github.com/robcowart/elastiflow/tree/master/kibana">https://github.com/robcowart/elastiflow/tree/master/kibana</a></p>



<p>コンテスト中のflow数はこのような形です。</p>



<figure class="wp-block-image"><img decoding="async" src="https://i.imgur.com/yEzXPis.png.webp" alt="" /></figure>



<p>監視基盤については以上です。</p>



<h2>終わりに</h2>



<p>今回k8sをどのように利活用したかについて説明しました。これが今回の我々の成果になります！<br>
前編・後編と説明しましたが面白く読んでもらえましたでしょうか？<br>
最後になりますが参加してくださった皆さん、スポンサーとしてリソース提供をしてくださったさくらインターネット様ありがとうございました。</p>



<p>もし今回の記事が皆さんがk8sを運用するにあたっての参考になれば幸いです。</p>
