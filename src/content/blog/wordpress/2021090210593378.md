---
title: "クラウドネイティブは難しい…"
description: "クラウドネイティブは難しい…"
tags: [ICTSC2021,未分類]
pubDate: 2021-09-02T10:59:13
slug: "2021/09/02/クラウドネイティブは難しい"
draft: false
renderer: "html"
sticky: false
---


<h2>概要</h2>



<p>この問題は前半フェーズと後半フェーズで別れています。<br>
問題環境は共通のものを使用しますが、採点時は前半と後半で別に採点を行います。<br>
よって、前半のみあるいは後半のみの提出の場合でも部分点を与える場合があります。</p>



<h3>前半フェーズ</h3>



<p>新入社員のCさんは研修でKubernetes on Openstackの環境を作るように上司のKさんから言われました。<br>
Cさんはkubesprayを使用して、KubernetesをOpenstack上で展開しようとしましたが、うまく展開出来ないようです。<br>
詳細には以下のようなエラーを吐いているようです。<br>
まずは、Cさんを助けてあげてください！</p>


<div class="wp-block-syntaxhighlighter-code "><pre><code>TASK &#91;kubernetes/preinstall : Update package management cache (APT)] *******************************************************
fatal: &#91;node2]: FAILED! =&gt; {&quot;changed&quot;: false, &quot;msg&quot;: &quot;Failed to update apt cache: unknown reason&quot;}
fatal: &#91;node1]: FAILED! =&gt; {&quot;changed&quot;: false, &quot;msg&quot;: &quot;Failed to update apt cache: unknown reason&quot;}</code></pre></div>


<h3>後半フェーズ</h3>



<p>Cさんの環境をうまく変更し、Kubernetesを展開することが出来ました。<br>
その後、Kさんに報告して環境のチェックをして頂いたところ、特定の状況下で以下のようにpod間の疎通が取れないということがわかりました。<br>
Cさんの代わりにパターンA・パターンBのどちらの場合でも疎通が取れるようにしてください。</p>


<div class="wp-block-syntaxhighlighter-code "><pre><code>//コンソール1
//コマンド以外の一部のログは省略してあります。
//以下の結果からnode1に展開したpod「shell1」のIPアドレスが10.233.96.3だということがわかる。
user@server01:~$ ssh -i ../.ssh/ictsc_rsa ubuntu@10.20.20.235
ubuntu@node1:~$ kubectl run shell1 -it --rm --image busybox -- sh --overrides='{ &quot;apiVersion&quot;: &quot;v1&quot;, &quot;spec&quot;: { &quot;nodename&quot;: &quot;node1&quot; } }'
If you don't see a command prompt, try pressing enter.
/ # hostname -i
10.233.96.3
/ #</code></pre></div>

<div class="wp-block-syntaxhighlighter-code "><pre><code>//パターンA:別nodeにpodが展開された場合
//コンソール2
//コマンド以外の一部のログは省略してあります。
//pingコマンドの結果からnode1のpod「shell1」にnode2のpod「shell2」から疎通が取れないことがわかる
ubuntu@node1:~$ kubectl run myshell2 -it --rm --image busybox -- sh --overrides='{ &quot;apiVersion&quot;: &quot;v1&quot;, &quot;spec&quot;: { &quot;nodename&quot;: &quot;node2&quot; } }'
If you don't see a command prompt, try pressing enter.
/ #
/ # ping 10.233.96.3
PING 10.233.96.3 (10.233.96.3): 56 data bytes
^C
--- 10.233.96.3 ping statistics ---
9 packets transmitted, 0 packets received, 100% packet loss
/ #</code></pre></div>

<div class="wp-block-syntaxhighlighter-code "><pre><code>//パターンB:同nodeにpodが展開された場合
//コンソール2 
//コマンド以外の一部のログは省略してあります。
//pingコマンドの結果からnode1のpod「shell1」にnode1のpod「shell2」から疎通が取れることがわかる
ubuntu@node1:~$ kubectl run myshell2 -it --rm --image busybox -- sh --overrides='{ &quot;apiVersion&quot;: &quot;v1&quot;, &quot;spec&quot;: { &quot;nodename&quot;: &quot;node1&quot; } }'
If you don't see a command prompt, try pressing enter.
/ # ping 10.233.96.3
PING 10.233.96.3 (10.233.96.3): 56 data bytes
64 bytes from 10.233.96.3: seq=0 ttl=63 time=0.576 ms
64 bytes from 10.233.96.3: seq=1 ttl=63 time=0.429 ms
^C
--- 10.233.96.3 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 0.429/0.502/0.576 ms
/ #</code></pre></div>


<h3>共通部分</h3>



<h4>kubernetes</h4>



<p>展開にはkubesprayを使用しようとしています。<br>
実際に展開する際にもkubesprayを使用してください。<br>
また、kubesprayを実行してKubernetesが展開できた場合は、VM「ictsc-master」にSSHで接続し、以下のコマンドを例としてkubectlが使用できるようにしてください。</p>


<div class="wp-block-syntaxhighlighter-code "><pre><code>user@server01:~$ ssh -i ../.ssh/ictsc_rsa ubuntu@&#91;ictsc-masterのFloating IP]
ubuntu@node1:~$ mkdir -p $HOME/.kube
ubuntu@node1:~$ sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
ubuntu@node1:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
ubuntu@node1:~$ export KUBECONFIG=$HOME/.kube/config</code></pre></div>


<h4>openstack</h4>



<p>Openstack(mirostack)上に以下のようにVMが2台存在します。<br>
2台ともstatusがSHUTOFFになっているため、server01の再起動時には<code>openstack server start [vm名]</code> で各VMを起動してください<br>
また、Dashboardの認証情報は以下のようになっています。</p>



<ul><li>username<ul><li>admin</li></ul></li><li>password<ul><li>QpqBUv1CrbkENSEPb5qE83cjTr7p59Hv</li></ul></li></ul>


<div class="wp-block-syntaxhighlighter-code "><pre><code>user@server01:~$ openstack server show ictsc-master
+-------------------------------------+----------------------------------------------------------+
| Field                               | Value                                                    |
+-------------------------------------+----------------------------------------------------------+
| OS-DCF:diskConfig                   | MANUAL                                                   |
| OS-EXT-AZ:availability_zone         | nova                                                     |
| OS-EXT-SRV-ATTR:host                | server01                                                 |
| OS-EXT-SRV-ATTR:hypervisor_hostname | server01                                                 |
| OS-EXT-SRV-ATTR:instance_name       | instance-00000002                                        |
| OS-EXT-STS:power_state              | Shutdown                                                 |
| OS-EXT-STS:task_state               | None                                                     |
| OS-EXT-STS:vm_state                 | stopped                                                  |
| OS-SRV-USG:launched_at              | 2021-08-07T06:11:32.000000                               |
| OS-SRV-USG:terminated_at            | None                                                     |
| accessIPv4                          |                                                          |
| accessIPv6                          |                                                          |
| addresses                           | test=192.168.222.222, 10.20.20.235                       |
| config_drive                        |                                                          |
| created                             | 2021-08-07T06:11:21Z                                     |
| flavor                              | m1.master (4a78eb71-ec25-4dc8-8d97-1955f618b260)         |
| hostId                              | fc20b879f84666d87915b0907fa95a8c0cf72e8518942f8645b0ed45 |
| id                                  | 7022faff-f9eb-49c7-a38b-72f10cded511                     |
| image                               | ubuntu (23575e0a-b508-4610-b2aa-fe5707db9d0b)            |
| key_name                            | mykey                                                    |
| name                                | ictsc-master                                             |
| project_id                          | 65081f8c8fac4442ba440d85c5861ae5                         |
| properties                          |                                                          |
| security_groups                     | name='master'                                            |
| status                              | SHUTOFF                                                  |
| updated                             | 2021-08-20T13:41:17Z                                     |
| user_id                             | f8f0af1a6abd458cace71a66b8062a16                         |
| volumes_attached                    |                                                          |
+-------------------------------------+----------------------------------------------------------+
user@server01:~$ openstack server show ictsc-worker
+-------------------------------------+----------------------------------------------------------+
| Field                               | Value                                                    |
+-------------------------------------+----------------------------------------------------------+
| OS-DCF:diskConfig                   | MANUAL                                                   |
| OS-EXT-AZ:availability_zone         | nova                                                     |
| OS-EXT-SRV-ATTR:host                | server01                                                 |
| OS-EXT-SRV-ATTR:hypervisor_hostname | server01                                                 |
| OS-EXT-SRV-ATTR:instance_name       | instance-00000001                                        |
| OS-EXT-STS:power_state              | Shutdown                                                 |
| OS-EXT-STS:task_state               | None                                                     |
| OS-EXT-STS:vm_state                 | stopped                                                  |
| OS-SRV-USG:launched_at              | 2021-08-07T06:11:34.000000                               |
| OS-SRV-USG:terminated_at            | None                                                     |
| accessIPv4                          |                                                          |
| accessIPv6                          |                                                          |
| addresses                           | test=192.168.222.157, 10.20.20.206                       |
| config_drive                        |                                                          |
| created                             | 2021-08-07T06:11:00Z                                     |
| flavor                              | m1.worker (f36b186c-8d6c-49e1-8546-250f9eed48db)         |
| hostId                              | fc20b879f84666d87915b0907fa95a8c0cf72e8518942f8645b0ed45 |
| id                                  | 133bd167-f0ad-4dea-8721-82d583bbde75                     |
| image                               | ubuntu (23575e0a-b508-4610-b2aa-fe5707db9d0b)            |
| key_name                            | mykey                                                    |
| name                                | ictsc-worker                                             |
| project_id                          | 65081f8c8fac4442ba440d85c5861ae5                         |
| properties                          |                                                          |
| security_groups                     | name='worker'                                            |
| status                              | SHUTOFF                                                  |
| updated                             | 2021-08-20T13:41:17Z                                     |
| user_id                             | f8f0af1a6abd458cace71a66b8062a16                         |
| volumes_attached                    |                                                          |
+-------------------------------------+----------------------------------------------------------+
user@server01:~$</code></pre></div>


<figure class="wp-block-image"><img decoding="async" src="https://i.imgur.com/3zwSN8n.png.webp" alt="" /></figure>



<p>以下、Openstack内のトポロジー図  <br>
</p>



<h2>前提条件</h2>



<ul><li>kubesprayディレクトリ内のファイルには変更は加えないでください</li><li>openstackは現在のノードから移動・再展開しないでください。</li><li>この問題は採点時に問題環境を確認しません。<ul><li>なので、この問題の解答は「なぜ、どこが、どうして、どうやって修正したのか」をレポートとして報告してください。</li></ul></li><li>提供された問題環境を現在の状態から必要以上にセキュリティレベルを下げないでください。<ul><li>もしも回答で下げる必要がある場合はレポート内に理由を記載してください。</li></ul></li></ul>



<h2>初期状態</h2>



<h3>前半フェーズ</h3>



<p>以下のコマンドを<code>~/kubespray</code> で実行しても途中でエラーを吐いて止まってしまう。</p>


<div class="wp-block-syntaxhighlighter-code "><pre><code>//コマンド
ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml -u ubuntu --private-key=~/.ssh/ictsc_rsa</code></pre></div>

<div class="wp-block-syntaxhighlighter-code "><pre><code>//エラー文
TASK &#91;kubernetes/preinstall : Update package management cache (APT)] *******************************************************
fatal: &#91;node2]: FAILED! =&gt; {&quot;changed&quot;: false, &quot;msg&quot;: &quot;Failed to update apt cache: unknown reason&quot;}
fatal: &#91;node1]: FAILED! =&gt; {&quot;changed&quot;: false, &quot;msg&quot;: &quot;Failed to update apt cache: unknown reason&quot;}</code></pre></div>


<h3>後半フェーズ</h3>



<p>構築したkubernetesの各ノードにpodが建った場合に、各ノード上のpod間で疎通が<strong>取れない</strong></p>



<ol><li>node1上のshell1からnode2上のshell2に対してpingを打った場合、疎通が<strong>取れない</strong></li><li>2. node1上のshell1からnode1上のshell1に対してpingを打った場合、疎通が取れる</li></ol>



<h2>終了状態</h2>



<h3>前半フェーズ</h3>



<p>kubesprayのすべてのTaskを実行してKubernetesが展開できている。<br>
また、node1(vm:ictsc-master)にSSHで接続し、<code>kubectl get nodes</code>を実行した際に正しくノードの情報が表示される。</p>



<h3>後半フェーズ</h3>



<p>構築したkubernetesの各ノードにpodが建った場合に、各ノード上のpod間で疎通が<strong>取れる</strong></p>



<ol><li>node1上のshell1からnode2上のshell2に対してpingを打った場合、疎通が<strong>取れる</strong></li><li>node1上のshell1からnode1上のshell1に対してpingを打った場合、疎通が取れる</li></ol>



<h2>解説</h2>



<p>こんにちは。この問題を担当したsiteです。本問題は、KubernetesやOpenstackについての知識、Linuxに関する基礎的な内容を問う問題となっていました。  <br>
以下、想定解法と共に解説をしていきます。  <br>
また、本問題は前半パートと後半パートから構成された問題となっていましたので、解説についても前半と後半に分けて解説していきます。  </p>



<h3>前半パート</h3>



<p>まず、はじめにOpenstack上のVMを起動します。  <br>
本問題では、microstack()を用いてOpenstackの環境を構築していましたが、<code>snap alias</code>を用いて<code>openstack</code>コマンドが使用できるようになっていました。  </p>


<div class="wp-block-syntaxhighlighter-code "><pre><code>user@server01:~$ openstack server start ictsc-master
user@server01:~$ openstack server start ictsc-worker</code></pre></div>


<p>次に、前半パートの想定解となるインターネットに出るための設定を入れます。  <br>
今回の問題環境では、カーネルパラメータ内で<code>net.ipv4.ip_forward</code> が0になっていることで、Openstack内のVMが外部ネットワークに接続する際に転送されるパケットがdropされるようになっていました。  <br>
なので、<code>sysctl -w</code>などを用いて有効にします。  </p>


<div class="wp-block-syntaxhighlighter-code "><pre><code>user@server01:~$ sudo sysctl -w net.ipv4.ip_forward=1
or 
user@server01:~$ sudo sh -c &quot;echo 'net.ipv4.ip_forward=1' &gt;&gt; /etc/sysctl.conf&quot;
などなど</code></pre></div>


<p>以上の設定をしたら、kubesprayを実行するだけです。  </p>


<div class="wp-block-syntaxhighlighter-code "><pre><code>user@server01:~$ ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml -u ubuntu --private-key=~/.ssh/ictsc_rsa</code></pre></div>


<p>また、問題文に書いてあるようにkubectlを使えるようにすることで、後半フェーズへの移行が可能になります。  </p>


<div class="wp-block-syntaxhighlighter-code "><pre><code>user@server01:~$ ssh -i ../.ssh/ictsc_rsa ubuntu@&#91;ictsc-masterのFloating IP]
ubuntu@node1:~$ mkdir -p $HOME/.kube
ubuntu@node1:~$ sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
ubuntu@node1:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
ubuntu@node1:~$ export KUBECONFIG=$HOME/.kube/config</code></pre></div>


<p>ここまでで前半フェーズは完了となります。  <br>
次にCLI上かDashboard上から、masterとworkerを以下のようなsecurity groupのルールにする。  <br>
また、ポートフォワードしてdashboardに接続する場合は下記のコマンドなどを使用します。  <br>
<code>ssh -t -p 22 user@踏み台 -L 8080:192.168.15.1:80</code></p>



<ul><li>ルール<ul><li><code>TCP/179</code><ul><li>Calico-nodeがBGPを確立するために必要<br>
//参考: https://docs.projectcalico.org/getting-started/kubernetes/requirements  </li></ul></li></ul></li></ul>



<p>これによって、同ノード間や異なるノード間において疎通性が確認することが出来ます。</p>
